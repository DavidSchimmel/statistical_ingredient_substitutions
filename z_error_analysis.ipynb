{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "R1MSUBS_TRAIN_PATH = os.path.abspath(\"./inputs/train_comments_subs.pkl\")\n",
    "R1MSUBS_VAL_PATH = os.path.abspath(\"./inputs/val_comments_subs.pkl\")\n",
    "R1MSUBS_TEST_PATH = os.path.abspath(\"./inputs/test_comments_subs.pkl\")\n",
    "\n",
    "IDX2WORD_PATH = os.path.abspath(\"./inputs/graph/idx_2_word.pkl\")\n",
    "WORD2IDX_PATH = os.path.abspath(\"./inputs/graph/word_2_idx.pkl\")\n",
    "\n",
    "with open(R1MSUBS_TRAIN_PATH, \"rb\") as train_file:\n",
    "    r1msubs_train = pickle.load(train_file)\n",
    "with open(R1MSUBS_VAL_PATH, \"rb\") as val_file:\n",
    "    r1msubs_val = pickle.load(val_file)\n",
    "with open(R1MSUBS_TEST_PATH, \"rb\") as test_file:\n",
    "    r1msubs_test = pickle.load(test_file)\n",
    "\n",
    "with open(IDX2WORD_PATH, \"rb\") as idx2word_file:\n",
    "    idx2word = pickle.load(idx2word_file)\n",
    "with open(WORD2IDX_PATH, \"rb\") as word2idx_file:\n",
    "    word2idx = pickle.load(word2idx_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '1%_fat_buttermilk'), (2, '1%_fat_cottage_cheese'), (3, '10%_cream'), (4, '100%_bran'), (5, '10_inch_flour_tortilla')]\n",
      "ground_beef\n"
     ]
    }
   ],
   "source": [
    "INGR_NODES_PATH = os.path.abspath(\"./inputs/graph/nodes_191120.csv\")\n",
    "\n",
    "rows = []\n",
    "with open(INGR_NODES_PATH, 'r') as file:\n",
    "    for line in file:\n",
    "        values = line.split(\",\")\n",
    "\n",
    "        rows.append(values)\n",
    "\n",
    "node_idx_2_name = {}\n",
    "for c, row in enumerate(rows):\n",
    "    if row[3] != \"ingredient\":\n",
    "        continue\n",
    "    node_idx_2_name[c] = row[1]\n",
    "\n",
    "print(list(node_idx_2_name.items())[:5])\n",
    "print(node_idx_2_name[2865])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inv_cooking'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m VOCABS_PATH \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./inputs/vocab_ingrs.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(VOCABS_PATH, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vocabs_file:\n\u001b[1;32m----> 4\u001b[0m     vocabs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(vocabs_file)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'inv_cooking'"
     ]
    }
   ],
   "source": [
    "VOCABS_PATH = os.path.abspath(\"./inputs/vocab_ingrs.pkl\")\n",
    "\n",
    "with open(VOCABS_PATH, \"rb\") as vocabs_file:\n",
    "    vocabs = pickle.load(vocabs_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subs = []\n",
    "for train_sample in r1msubs_train:\n",
    "    sub = train_sample[\"subs\"]\n",
    "    if sub not in train_subs:\n",
    "        train_subs.append(sub)\n",
    "\n",
    "val_subs = []\n",
    "for val_sample in r1msubs_val:\n",
    "    sub = val_sample[\"subs\"]\n",
    "    if sub not in val_subs:\n",
    "        val_subs.append(sub)\n",
    "\n",
    "test_subs = []\n",
    "for test_sample in r1msubs_test:\n",
    "    sub = test_sample[\"subs\"]\n",
    "    if sub not in test_subs:\n",
    "        test_subs.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subs_not_in_train = []\n",
    "for test_sub in test_subs:\n",
    "    if test_sub not in train_subs and test_sub not in test_subs_not_in_train:\n",
    "        test_subs_not_in_train.append(test_sub)\n",
    "\n",
    "test_sources_not_in_train = []\n",
    "train_sources = [pair[0] for pair in train_subs]\n",
    "for test_sub in test_subs:\n",
    "    if test_sub[0] not in train_sources and test_sub[0] not in test_sources_not_in_train:\n",
    "        test_sources_not_in_train.append(test_sub)\n",
    "\n",
    "test_targets_not_in_train = []\n",
    "train_targets = [pair[1] for pair in train_subs]\n",
    "for test_sub in test_subs:\n",
    "    if test_sub[0] not in train_targets and test_sub[0] not in test_targets_not_in_train:\n",
    "        test_targets_not_in_train.append(test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_subs_not_in_train = []\n",
    "for val_sub in val_subs:\n",
    "    if val_sub not in train_subs and val_sub not in val_subs_not_in_train:\n",
    "        val_subs_not_in_train.append(val_sub)\n",
    "\n",
    "val_sources_not_in_train = []\n",
    "train_sources = [pair[0] for pair in train_subs]\n",
    "for val_sub in val_subs:\n",
    "    if val_sub[0] not in train_sources and val_sub[0] not in val_sources_not_in_train:\n",
    "        val_sources_not_in_train.append(val_sub)\n",
    "\n",
    "val_targets_not_in_train = []\n",
    "train_targets = [pair[1] for pair in train_subs]\n",
    "for val_sub in val_subs:\n",
    "    if val_sub[0] not in train_targets and val_sub[0] not in val_targets_not_in_train:\n",
    "        val_targets_not_in_train.append(val_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***samples***\n",
      "49044\n",
      "10729\n",
      "10747\n",
      "***subs***\n",
      "23421\n",
      "6895\n",
      "6831\n",
      "***test***\n",
      "3778\n",
      "323\n",
      "675\n",
      "***val***\n",
      "3828\n",
      "286\n",
      "630\n"
     ]
    }
   ],
   "source": [
    "print(\"***samples***\")\n",
    "print(len(r1msubs_train))\n",
    "print(len(r1msubs_val))\n",
    "print(len(r1msubs_test))\n",
    "print(\"***subs***\")\n",
    "print(len(train_subs))\n",
    "print(len(val_subs))\n",
    "print(len(test_subs))\n",
    "print(\"***test***\")\n",
    "print(len(test_subs_not_in_train))\n",
    "print(len(test_sources_not_in_train))\n",
    "print(len(test_targets_not_in_train))\n",
    "print(\"***val***\")\n",
    "print(len(val_subs_not_in_train))\n",
    "print(len(val_sources_not_in_train))\n",
    "print(len(val_targets_not_in_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "324\n",
      "291\n",
      "3881\n",
      "3958\n"
     ]
    }
   ],
   "source": [
    "# get the number of test and val samples with unseen sources\n",
    "\n",
    "test_samples_with_unseen_sources = []\n",
    "test_samples_with_unseen_subs = []\n",
    "for test_sub in r1msubs_test:\n",
    "    if test_sub[\"subs\"][0] in [sub[0] for sub in test_sources_not_in_train]:\n",
    "        test_samples_with_unseen_sources.append(test_sub)\n",
    "    if test_sub[\"subs\"] in test_subs_not_in_train:\n",
    "        test_samples_with_unseen_subs.append(test_sub)\n",
    "\n",
    "val_samples_with_unseen_sources = []\n",
    "val_samples_with_unseen_subs = []\n",
    "for val_sub in r1msubs_val:\n",
    "    if val_sub[\"subs\"][0] in [sub[0] for sub in val_sources_not_in_train]:\n",
    "        val_samples_with_unseen_sources.append(val_sub)\n",
    "    if val_sub[\"subs\"] in val_subs_not_in_train:\n",
    "        val_samples_with_unseen_subs.append(val_sub)\n",
    "\n",
    "print(len(test_samples_with_unseen_sources))\n",
    "print(len(val_samples_with_unseen_sources))\n",
    "print(len(test_samples_with_unseen_subs))\n",
    "print(len(val_samples_with_unseen_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the number and fraction of seen/unseen samples in the test and train splits for each of the data sets? (also recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcfive_train_0 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/train_comments_subs_0.pkl\")\n",
    "arcfive_test_0 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/test_comments_subs_0.pkl\")\n",
    "arcfive_train_1 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/train_comments_subs_1.pkl\")\n",
    "arcfive_test_1 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/test_comments_subs_1.pkl\")\n",
    "arcfive_train_2 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/train_comments_subs_2.pkl\")\n",
    "arcfive_test_2 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/test_comments_subs_2.pkl\")\n",
    "arcfive_train_3 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/train_comments_subs_3.pkl\")\n",
    "arcfive_test_3 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/test_comments_subs_3.pkl\")\n",
    "arcfive_train_4 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/train_comments_subs_4.pkl\")\n",
    "arcfive_test_4 = os.path.abspath(\"./outputs/new_comments/arcelik_five_fold/test_comments_subs_4.pkl\")\n",
    "\n",
    "arcfive_splits = [\n",
    "    (\n",
    "        arcfive_train_0,\n",
    "        arcfive_test_0\n",
    "    ),\n",
    "    (\n",
    "        arcfive_train_1,\n",
    "        arcfive_test_1\n",
    "    ),\n",
    "    (\n",
    "        arcfive_train_2,\n",
    "        arcfive_test_2\n",
    "    ),\n",
    "    (\n",
    "        arcfive_train_3,\n",
    "        arcfive_test_3\n",
    "    ),\n",
    "    (\n",
    "        arcfive_train_4,\n",
    "        arcfive_test_4\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "def get_unseen_pairs_and_recipes(train_path, val_or_test_path):\n",
    "    with open(train_path, \"rb\") as train_file:\n",
    "        train_comments = pickle.load(train_file)\n",
    "    with open(val_or_test_path, \"rb\") as other_file:\n",
    "        other_comments = pickle.load(other_file)\n",
    "\n",
    "    n_train_samples = len(train_comments)\n",
    "    n_other_samples = len(other_comments)\n",
    "\n",
    "    seen_subs = set([comment[\"subs\"] for comment in train_comments])\n",
    "    seen_recipe_ids = set([comment[\"id\"] for comment in train_comments])\n",
    "\n",
    "    distinct_other_subs = set([comment[\"subs\"] for comment in other_comments])\n",
    "    distinct_other_ids = set([comment[\"id\"] for comment in other_comments])\n",
    "\n",
    "\n",
    "    unseen_subs = []\n",
    "    unseen_recipe_ids = []\n",
    "    for comment in other_comments:\n",
    "        sub = comment[\"subs\"]\n",
    "        id = comment[\"id\"]\n",
    "\n",
    "        if sub not in seen_subs and sub not in unseen_subs:\n",
    "            unseen_subs.append(sub)\n",
    "        if id not in seen_recipe_ids and id not in unseen_recipe_ids:\n",
    "            unseen_recipe_ids.append(id)\n",
    "\n",
    "\n",
    "    n_samples_with_unseen_subs = 0\n",
    "    n_samples_with_unseen_sources = 0\n",
    "    seen_sources = [sub[0] for sub in seen_subs]\n",
    "\n",
    "    for comment in other_comments:\n",
    "        if comment[\"subs\"] not in seen_subs:\n",
    "            n_samples_with_unseen_subs += 1\n",
    "        if comment[\"subs\"][0] not in seen_sources:\n",
    "            n_samples_with_unseen_sources += 1\n",
    "\n",
    "    return unseen_subs, unseen_recipe_ids, distinct_other_subs, distinct_other_ids, n_train_samples, n_other_samples, n_samples_with_unseen_subs, n_samples_with_unseen_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold 0\n",
      "nr unseen subs: 21\n",
      "nr unseen recipe: 60\n",
      "nr distinct other subs: 75\n",
      "nr distinct other ids: 123\n",
      "nr total train samples: 524\n",
      "nr total other samples: 132\n",
      "nr of samples with unseen subs: 22\n",
      "nr of samples with unseen sources: 14\n",
      "*****\n",
      "for fold 1\n",
      "nr unseen subs: 19\n",
      "nr unseen recipe: 57\n",
      "nr distinct other subs: 77\n",
      "nr distinct other ids: 116\n",
      "nr total train samples: 525\n",
      "nr total other samples: 131\n",
      "nr of samples with unseen subs: 20\n",
      "nr of samples with unseen sources: 6\n",
      "*****\n",
      "for fold 2\n",
      "nr unseen subs: 16\n",
      "nr unseen recipe: 69\n",
      "nr distinct other subs: 77\n",
      "nr distinct other ids: 122\n",
      "nr total train samples: 525\n",
      "nr total other samples: 131\n",
      "nr of samples with unseen subs: 18\n",
      "nr of samples with unseen sources: 7\n",
      "*****\n",
      "for fold 3\n",
      "nr unseen subs: 17\n",
      "nr unseen recipe: 59\n",
      "nr distinct other subs: 75\n",
      "nr distinct other ids: 121\n",
      "nr total train samples: 525\n",
      "nr total other samples: 131\n",
      "nr of samples with unseen subs: 17\n",
      "nr of samples with unseen sources: 6\n",
      "*****\n",
      "for fold 4\n",
      "nr unseen subs: 25\n",
      "nr unseen recipe: 50\n",
      "nr distinct other subs: 83\n",
      "nr distinct other ids: 122\n",
      "nr total train samples: 525\n",
      "nr total other samples: 131\n",
      "nr of samples with unseen subs: 27\n",
      "nr of samples with unseen sources: 17\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "unseen_per_split = [get_unseen_pairs_and_recipes(k[0], k[1]) for k in arcfive_splits]\n",
    "\n",
    "for i, unseen in enumerate(unseen_per_split):\n",
    "    print(f\"for fold {i}\")\n",
    "    print(f\"nr unseen subs: {len(unseen[0])}\")\n",
    "    print(f\"nr unseen recipe: {len(unseen[1])}\")\n",
    "    print(f\"nr distinct other subs: {len(unseen[2])}\")\n",
    "    print(f\"nr distinct other ids: {len(unseen[3])}\")\n",
    "    print(f\"nr total train samples: {unseen[4]}\")\n",
    "    print(f\"nr total other samples: {unseen[5]}\")\n",
    "    print(f\"nr of samples with unseen subs: {unseen[6]}\")\n",
    "    print(f\"nr of samples with unseen sources: {unseen[7]}\")\n",
    "    print(\"*****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for fold val\n",
      "nr unseen subs: 3828\n",
      "nr unseen recipe: 6670\n",
      "nr distinct other subs: 6895\n",
      "nr distinct other ids: 6670\n",
      "nr total train samples: 49044\n",
      "nr total other samples: 10729\n",
      "*****\n",
      "for fold test\n",
      "nr unseen subs: 3778\n",
      "nr unseen recipe: 6662\n",
      "nr distinct other subs: 6831\n",
      "nr distinct other ids: 6662\n",
      "nr total train samples: 49044\n",
      "nr total other samples: 10747\n",
      "*****\n"
     ]
    }
   ],
   "source": [
    "r1msubs_train = os.path.abspath(\"./inputs/train_comments_subs.pkl\")\n",
    "r1msubs_val = os.path.abspath(\"./inputs/val_comments_subs.pkl\")\n",
    "r1msubs_test = os.path.abspath(\"./inputs/test_comments_subs.pkl\")\n",
    "\n",
    "r1m_val_nums = get_unseen_pairs_and_recipes(r1msubs_train, r1msubs_val)\n",
    "r1m_test_nums = get_unseen_pairs_and_recipes(r1msubs_train, r1msubs_test)\n",
    "\n",
    "for i, unseen in enumerate([r1m_val_nums, r1m_test_nums]):\n",
    "    label = \"val\" if i == 0 else \"test\"\n",
    "\n",
    "    print(f\"for fold {label}\")\n",
    "    print(f\"nr unseen subs: {len(unseen[0])}\")\n",
    "    print(f\"nr unseen recipe: {len(unseen[1])}\")\n",
    "    print(f\"nr distinct other subs: {len(unseen[2])}\")\n",
    "    print(f\"nr distinct other ids: {len(unseen[3])}\")\n",
    "    print(f\"nr total train samples: {unseen[4]}\")\n",
    "    print(f\"nr total other samples: {unseen[5]}\")\n",
    "    print(\"*****\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the diversity and mean diversity for each of the 5 fold results set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_diversity(rows):\n",
    "    all_ranks_for_sources = {}\n",
    "    for row in rows:\n",
    "        source = row[0]\n",
    "        top_rank_pred = row[3]\n",
    "\n",
    "        if source not in list(all_ranks_for_sources.keys()):\n",
    "            all_ranks_for_sources[source] = []\n",
    "        all_ranks_for_sources[source].append(top_rank_pred)\n",
    "\n",
    "    undiverse_top_rank_count = 0\n",
    "    undiverse_top_rank_count_with_more_than_10_recipes = 0\n",
    "    abs_div_sum = 0\n",
    "    distinct_source_count = 0\n",
    "\n",
    "    for source, all_targets in all_ranks_for_sources.items():\n",
    "        distinct_targets = []\n",
    "        for target in all_targets:\n",
    "            if target not in distinct_targets:\n",
    "                distinct_targets.append(target)\n",
    "\n",
    "        n_unique_preds = len(distinct_targets)\n",
    "        distinct_source_count += 1\n",
    "        abs_div_sum += n_unique_preds\n",
    "        if n_unique_preds <= 1:\n",
    "            undiverse_top_rank_count += 1\n",
    "            if len(all_targets) > 10:\n",
    "                undiverse_top_rank_count_with_more_than_10_recipes += 1\n",
    "\n",
    "    avg_div = abs_div_sum / distinct_source_count\n",
    "\n",
    "    return avg_div, undiverse_top_rank_count, undiverse_top_rank_count_with_more_than_10_recipes\n",
    "\n",
    "\n",
    "    # print(f\"average top rank diversity: {abs_div_sum / distinct_source_count}\")\n",
    "    # print(f\"total number of distinct substitution sources: {abs_div_sum}\")\n",
    "    # print(f\"number of substitution sources which have only 1 recommended ingredient: {undiverse_top_rank_count}\")\n",
    "    # print(f\"number of substitution sources which have only 1 recommended ingreident and which is applied to more than 10 recipes: {undiverse_top_rank_count_with_more_than_10_recipes}\")\n",
    "\n",
    "def load_rows(file_path):\n",
    "    # val_ranks_path = os.path.abspath(\"./val_ranks.txt\")\n",
    "\n",
    "    rows = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "\n",
    "            rows.append(values)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "D(1)\n",
      "  baseline: 1.0510317705801777$\\pm$0.03371856702877994\n",
      "  contrastive: 1.0392366777534041$\\pm$0.02416547129557241\n",
      "  ring: 1.0553575079630535$\\pm$0.03374610129495772\n",
      "  ingredients: 1.0527557445202285$\\pm$0.049658565895570383\n",
      "  actions: 1.071915978457628$\\pm$0.036535711643531435\n",
      "  flow: 1.0667885433884376$\\pm$0.05315412528271684\n",
      "***\n",
      "Recipes with 1 sub pair and more than 10 recipes\n",
      "  baseline: 0.2$\\pm$0.4472135954999579\n",
      "  contrastive: 0.0$\\pm$0.0\n",
      "  ring: 0.0$\\pm$0.0\n",
      "  ingredients: 0.2$\\pm$0.4472135954999579\n",
      "  actions: 0.2$\\pm$0.4472135954999579\n",
      "  flow: 0.2$\\pm$0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "arcfive_base_path = os.path.abspath(\"./arc_five\")\n",
    "\n",
    "ds_baseline = []\n",
    "ds_contrastive = []\n",
    "ds_ring = []\n",
    "ds_ingredients = []\n",
    "ds_actions = []\n",
    "ds_flow = []\n",
    "\n",
    "dgt_baseline = []\n",
    "dgt_contrastive = []\n",
    "dgt_ring = []\n",
    "dgt_ingredients = []\n",
    "dgt_actions = []\n",
    "dgt_flow = []\n",
    "\n",
    "for i in range(5):\n",
    "    baseline_suf = f\"{i}/baseline_{i}_500/test_ranks.txt\"\n",
    "    contrastive_suf = f\"{i}/negative_{i}_500/test_ranks.txt\"\n",
    "    ring_suf = f\"{i}/ring_{i}_500/test_ranks.txt\"\n",
    "    ingredients_suf = f\"{i}/ingredients_{i}_500/test_ranks.txt\"\n",
    "    actions_suf = f\"{i}/actions_{i}_500/test_ranks.txt\"\n",
    "    flow_suf = f\"{i}/flow_{i}_500/test_ranks.txt\"\n",
    "\n",
    "    baseline_path = os.path.join(arcfive_base_path, baseline_suf)\n",
    "    contrastive_path = os.path.join(arcfive_base_path, contrastive_suf)\n",
    "    ring_path = os.path.join(arcfive_base_path, ring_suf)\n",
    "    ingredients_path = os.path.join(arcfive_base_path, ingredients_suf)\n",
    "    actions_path = os.path.join(arcfive_base_path, actions_suf)\n",
    "    flow_path = os.path.join(arcfive_base_path, flow_suf)\n",
    "\n",
    "    baseline_rows = load_rows(baseline_path)\n",
    "    contrastive_rows = load_rows(contrastive_path)\n",
    "    ring_rows = load_rows(ring_path)\n",
    "    ingredients_rows = load_rows(ingredients_path)\n",
    "    actions_rows = load_rows(actions_path)\n",
    "    flow_rows = load_rows(flow_path)\n",
    "\n",
    "    baseline_div, baseline_d1, baseline_dgt = calc_diversity(baseline_rows)\n",
    "    contrastive_div, contrastive_d1, contrastive_dgt = calc_diversity(contrastive_rows)\n",
    "    ring_div, ring_d1, ring_dgt = calc_diversity(ring_rows)\n",
    "    ingredients_div, ingredients_d1, ingredients_dgt = calc_diversity(ingredients_rows)\n",
    "    actions_div, actions_d1, actions_dgt = calc_diversity(actions_rows)\n",
    "    flow_div, flow_d1, flow_dgt = calc_diversity(flow_rows)\n",
    "\n",
    "    ds_baseline.append(baseline_div)\n",
    "    ds_contrastive.append(contrastive_div)\n",
    "    ds_ring.append(ring_div)\n",
    "    ds_ingredients.append(ingredients_div)\n",
    "    ds_actions.append(actions_div)\n",
    "    ds_flow.append(flow_div)\n",
    "\n",
    "    dgt_baseline.append(baseline_dgt)\n",
    "    dgt_contrastive.append(contrastive_dgt)\n",
    "    dgt_ring.append(ring_dgt)\n",
    "    dgt_ingredients.append(ingredients_dgt)\n",
    "    dgt_actions.append(actions_dgt)\n",
    "    dgt_flow.append(flow_dgt)\n",
    "\n",
    "print(\"***\")\n",
    "print(\"D(1)\")\n",
    "print(f\"  baseline: {statistics.fmean(ds_baseline)}$\\pm${statistics.stdev(ds_baseline)}\")\n",
    "print(f\"  contrastive: {statistics.fmean(ds_contrastive)}$\\pm${statistics.stdev(ds_contrastive)}\")\n",
    "print(f\"  ring: {statistics.fmean(ds_ring)}$\\pm${statistics.stdev(ds_ring)}\")\n",
    "print(f\"  ingredients: {statistics.fmean(ds_ingredients)}$\\pm${statistics.stdev(ds_ingredients)}\")\n",
    "print(f\"  actions: {statistics.fmean(ds_actions)}$\\pm${statistics.stdev(ds_actions)}\")\n",
    "print(f\"  flow: {statistics.fmean(ds_flow)}$\\pm${statistics.stdev(ds_flow)}\")\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Recipes with 1 sub pair and more than 10 recipes\")\n",
    "print(f\"  baseline: {statistics.fmean(dgt_baseline)}$\\pm${statistics.stdev(dgt_baseline)}\")\n",
    "print(f\"  contrastive: {statistics.fmean(dgt_contrastive)}$\\pm${statistics.stdev(dgt_contrastive)}\")\n",
    "print(f\"  ring: {statistics.fmean(dgt_ring)}$\\pm${statistics.stdev(dgt_ring)}\")\n",
    "print(f\"  ingredients: {statistics.fmean(dgt_ingredients)}$\\pm${statistics.stdev(dgt_ingredients)}\")\n",
    "print(f\"  actions: {statistics.fmean(dgt_actions)}$\\pm${statistics.stdev(dgt_actions)}\")\n",
    "print(f\"  flow: {statistics.fmean(dgt_flow)}$\\pm${statistics.stdev(dgt_flow)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity for ArcSubs with only unseen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***\n",
      "D(1)\n",
      "  baseline: 1.0337552742616032$\\pm$0.03185584149903265\n",
      "  contrastive: 1.0126582278481013$\\pm$0.012658227848101222\n",
      "  ring: 1.0379746835443038$\\pm$0.012658227848101333\n",
      "  ingredients: 1.0379746835443038$\\pm$0.021924693766694637\n",
      "  actions: 1.0421940928270041$\\pm$0.02635020252488768\n",
      "  flow: 1.0168776371308017$\\pm$0.014616462511129715\n",
      "***\n",
      "Recipes with 1 sub pair and more than 10 recipes\n",
      "  baseline: 0.0$\\pm$0.0\n",
      "  contrastive: 0.0$\\pm$0.0\n",
      "  ring: 0.0$\\pm$0.0\n",
      "  ingredients: 0.0$\\pm$0.0\n",
      "  actions: 0.0$\\pm$0.0\n",
      "  flow: 0.0$\\pm$0.0\n"
     ]
    }
   ],
   "source": [
    "# for set with unly unseen samples:\n",
    "arcfive_base_path = os.path.abspath(\"C:/Users/David/project/gismo/out/arc_unseen\")\n",
    "\n",
    "ds_baseline = []\n",
    "ds_contrastive = []\n",
    "ds_ring = []\n",
    "ds_ingredients = []\n",
    "ds_actions = []\n",
    "ds_flow = []\n",
    "\n",
    "dgt_baseline = []\n",
    "dgt_contrastive = []\n",
    "dgt_ring = []\n",
    "dgt_ingredients = []\n",
    "dgt_actions = []\n",
    "dgt_flow = []\n",
    "\n",
    "for i in range(1, 4):\n",
    "    baseline_suf = f\"baseline_{i}_500/test_ranks.txt\"\n",
    "    contrastive_suf = f\"negative_sampling_{i}_500/test_ranks.txt\"\n",
    "    ring_suf = f\"ring_{i}_500/test_ranks.txt\"\n",
    "    ingredients_suf = f\"ingredients_{i}_500/test_ranks.txt\"\n",
    "    actions_suf = f\"simple_instructions_{i}_500/test_ranks.txt\"\n",
    "    flow_suf = f\"complex_instructions_{i}_500/test_ranks.txt\"\n",
    "\n",
    "    baseline_path = os.path.join(arcfive_base_path, baseline_suf)\n",
    "    contrastive_path = os.path.join(arcfive_base_path, contrastive_suf)\n",
    "    ring_path = os.path.join(arcfive_base_path, ring_suf)\n",
    "    ingredients_path = os.path.join(arcfive_base_path, ingredients_suf)\n",
    "    actions_path = os.path.join(arcfive_base_path, actions_suf)\n",
    "    flow_path = os.path.join(arcfive_base_path, flow_suf)\n",
    "\n",
    "    baseline_rows = load_rows(baseline_path)\n",
    "    contrastive_rows = load_rows(contrastive_path)\n",
    "    ring_rows = load_rows(ring_path)\n",
    "    ingredients_rows = load_rows(ingredients_path)\n",
    "    actions_rows = load_rows(actions_path)\n",
    "    flow_rows = load_rows(flow_path)\n",
    "\n",
    "    baseline_div, baseline_d1, baseline_dgt = calc_diversity(baseline_rows)\n",
    "    contrastive_div, contrastive_d1, contrastive_dgt = calc_diversity(contrastive_rows)\n",
    "    ring_div, ring_d1, ring_dgt = calc_diversity(ring_rows)\n",
    "    ingredients_div, ingredients_d1, ingredients_dgt = calc_diversity(ingredients_rows)\n",
    "    actions_div, actions_d1, actions_dgt = calc_diversity(actions_rows)\n",
    "    flow_div, flow_d1, flow_dgt = calc_diversity(flow_rows)\n",
    "\n",
    "    ds_baseline.append(baseline_div)\n",
    "    ds_contrastive.append(contrastive_div)\n",
    "    ds_ring.append(ring_div)\n",
    "    ds_ingredients.append(ingredients_div)\n",
    "    ds_actions.append(actions_div)\n",
    "    ds_flow.append(flow_div)\n",
    "\n",
    "    dgt_baseline.append(baseline_dgt)\n",
    "    dgt_contrastive.append(contrastive_dgt)\n",
    "    dgt_ring.append(ring_dgt)\n",
    "    dgt_ingredients.append(ingredients_dgt)\n",
    "    dgt_actions.append(actions_dgt)\n",
    "    dgt_flow.append(flow_dgt)\n",
    "\n",
    "print(\"***\")\n",
    "print(\"D(1)\")\n",
    "print(f\"  baseline: {statistics.fmean(ds_baseline)}$\\pm${statistics.stdev(ds_baseline)}\")\n",
    "print(f\"  contrastive: {statistics.fmean(ds_contrastive)}$\\pm${statistics.stdev(ds_contrastive)}\")\n",
    "print(f\"  ring: {statistics.fmean(ds_ring)}$\\pm${statistics.stdev(ds_ring)}\")\n",
    "print(f\"  ingredients: {statistics.fmean(ds_ingredients)}$\\pm${statistics.stdev(ds_ingredients)}\")\n",
    "print(f\"  actions: {statistics.fmean(ds_actions)}$\\pm${statistics.stdev(ds_actions)}\")\n",
    "print(f\"  flow: {statistics.fmean(ds_flow)}$\\pm${statistics.stdev(ds_flow)}\")\n",
    "\n",
    "print(\"***\")\n",
    "print(\"Recipes with 1 sub pair and more than 10 recipes\")\n",
    "print(f\"  baseline: {statistics.fmean(dgt_baseline)}$\\pm${statistics.stdev(dgt_baseline)}\")\n",
    "print(f\"  contrastive: {statistics.fmean(dgt_contrastive)}$\\pm${statistics.stdev(dgt_contrastive)}\")\n",
    "print(f\"  ring: {statistics.fmean(dgt_ring)}$\\pm${statistics.stdev(dgt_ring)}\")\n",
    "print(f\"  ingredients: {statistics.fmean(dgt_ingredients)}$\\pm${statistics.stdev(dgt_ingredients)}\")\n",
    "print(f\"  actions: {statistics.fmean(dgt_actions)}$\\pm${statistics.stdev(dgt_actions)}\")\n",
    "print(f\"  flow: {statistics.fmean(dgt_flow)}$\\pm${statistics.stdev(dgt_flow)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we find some interesting recipes that show that diversity would be good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "# get all the subs pairs that had only one sub target with their recipes\n",
    "\n",
    "gismo_baseline_val_rows_path = os.path.abspath(\"./outputs/gismo_baseline_results/val_ranks.txt\")\n",
    "\n",
    "gismo_baseline_rows = load_rows(gismo_baseline_val_rows_path)\n",
    "\n",
    "recs_per_source = {}\n",
    "for row_idx, row in enumerate(gismo_baseline_rows):\n",
    "    source = row[0]\n",
    "    gt_sub = row[1]\n",
    "    rec_rank = row[2]\n",
    "    top_rec_sub = row[3] # if you swap this out with the full ranks, that file only has the source and then the top ranked rec and all ranked recs starting from the top ranked rec (again)\n",
    "    if source not in list(recs_per_source.keys()):\n",
    "        recs_per_source[source] = []\n",
    "    recs_per_source[source].append(top_rec_sub)\n",
    "\n",
    "distinct_top_subs_for_common_sources = {}\n",
    "for source, targets in recs_per_source.items():\n",
    "    if len(targets) < 10:\n",
    "        continue\n",
    "    if source not in list(distinct_top_subs_for_common_sources.keys()):\n",
    "        distinct_top_subs_for_common_sources[source] = []\n",
    "    for target in set(targets):\n",
    "        distinct_top_subs_for_common_sources[source].append(target)\n",
    "\n",
    "undiverse_sources_with_target = {a:b for a, b in list(distinct_top_subs_for_common_sources.items()) if len(b) == 1}\n",
    "print(len(undiverse_sources_with_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ground_clove', ['allspice']]\n",
      "['margarine', ['butter']]\n",
      "['maple_syrup', ['honey_syrup']]\n",
      "['lime_juice', ['lemon_juice']]\n",
      "['cream', ['milk']]\n",
      "['olive_oil', ['butter']]\n",
      "['sour_cream', ['yogurt']]\n",
      "['vanilla_extract', ['almond_extract']]\n",
      "['parsley', ['cilantro']]\n",
      "['sweet_potato', ['yam']]\n",
      "['chip', ['green_onion']]\n",
      "['salt', ['garlic_salt']]\n",
      "['lemon', ['lime']]\n",
      "['plain_yogurt', ['sour_cream']]\n",
      "['half_and_half', ['milk']]\n",
      "['walnut', ['pecan']]\n",
      "['lean_ground_beef', ['ground_turkey']]\n",
      "['cottage_cheese', ['ricotta_cheese']]\n",
      "['extra_virgin_olive_oil', ['butter']]\n",
      "['vegetable_broth', ['chicken_broth']]\n",
      "['applesauce', ['oil']]\n",
      "['pecan', ['walnut']]\n",
      "['ground_beef', ['ground_turkey']]\n",
      "['stewed_tomato', ['rotel_tomato']]\n",
      "['egg', ['egg_white']]\n",
      "['cinnamon', ['pumpkin_pie_spice']]\n",
      "['ground_pork', ['ground_turkey']]\n",
      "['lettuce', ['spinach']]\n",
      "['garlic_clove', ['garlic_powder']]\n",
      "['corn_syrup', ['honey_syrup']]\n",
      "['flour_tortilla', ['corn_tortilla']]\n",
      "['apple', ['pear']]\n",
      "['garlic_salt', ['garlic_powder']]\n",
      "['lemon_juice', ['lime_juice']]\n",
      "['fish_sauce', ['soy_sauce']]\n",
      "['almond_extract', ['vanilla_extract']]\n",
      "['shortening', ['butter']]\n",
      "['green_chilies', ['jalapeno']]\n",
      "['cilantro', ['parsley']]\n",
      "['pineapple_juice', ['orange_juice']]\n",
      "['tomato_paste', ['ketchup']]\n",
      "['cooked_ham', ['bacon']]\n",
      "['apple_cider', ['apple_juice']]\n",
      "['dry_white_wine', ['chicken_broth']]\n",
      "['chicken_bouillon_cube', ['chicken_broth']]\n",
      "['seasoning_salt', ['garlic_salt']]\n",
      "['orange_zest', ['lemon_zest']]\n",
      "['kale', ['spinach']]\n",
      "['low_fat_cottage_cheese', ['ricotta_cheese']]\n",
      "['currant', ['raisin']]\n",
      "['cherry_tomato', ['grape_tomato']]\n",
      "['feta_cheese', ['goat_cheese']]\n",
      "['vanilla', ['almond_extract']]\n",
      "['low_fat_sour_cream', ['yogurt']]\n",
      "['beef_broth', ['chicken_broth']]\n",
      "['vegetable_stock', ['chicken_broth']]\n",
      "['ground_cinnamon', ['pumpkin_pie_spice']]\n",
      "['ham', ['bacon']]\n",
      "['fresh_cilantro', ['parsley']]\n",
      "['crushed_red_pepper_flake', ['cayenne_pepper']]\n",
      "['golden_raisin', ['dried_cranberry']]\n",
      "['pear', ['apple']]\n",
      "['white_wine', ['chicken_broth']]\n",
      "['thyme', ['oregano']]\n",
      "['beef_bouillon_cube', ['beef_broth']]\n",
      "['corn_tortilla', ['flour_tortilla']]\n",
      "['yellow_cake_mix', ['cupcake']]\n",
      "['diced_tomato', ['rotel_tomato']]\n",
      "['heavy_whipping_cream', ['milk']]\n",
      "['half_and_half_cream', ['milk']]\n",
      "['peanut_butter', ['almond_butter']]\n",
      "['dry_breadcrumb', ['ritz_cracker']]\n",
      "['snow_pea', ['sugar_snap_pea']]\n",
      "['pumpkin', ['butternut_squash']]\n",
      "['nutmeg', ['allspice']]\n",
      "['light_corn_syrup', ['honey_syrup']]\n",
      "['pine_nut', ['walnut']]\n",
      "['shrimp', ['chicken']]\n",
      "['kosher_salt', ['sea_salt']]\n",
      "['tomato_juice', ['tomato_sauce']]\n",
      "['pimento', ['red_pepper']]\n"
     ]
    }
   ],
   "source": [
    "bad_sub_pair_names = []\n",
    "for source, distinct_targets in undiverse_sources_with_target.items():\n",
    "    target_names = [node_idx_2_name[int(distinct_target)] for distinct_target in distinct_targets if int(distinct_target) in list(node_idx_2_name.keys())]\n",
    "    source_name = node_idx_2_name[int(source)]\n",
    "    bad_sub_pair_names.append([source_name, target_names])\n",
    "\n",
    "for names in bad_sub_pair_names:\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# let's compare with the actions graph recommendations for the recipe1msubs dataset\n",
    "\n",
    "action_graph_val_rows_path = os.path.abspath(\"./a_mode_results_inspections/simpleInstructionContext/val_ranks.txt\")\n",
    "\n",
    "action_graph_rows = load_rows(action_graph_val_rows_path)\n",
    "\n",
    "action_recs_per_former_undiverse_source = {}\n",
    "for row_idx, row in enumerate(action_graph_rows):\n",
    "    source = row[0]\n",
    "    gt_sub = row[1]\n",
    "    rec_rank = row[2]\n",
    "    top_rec_sub = row[3] # if you swap this out with the full ranks, that file only has the source and then the top ranked rec and all ranked recs starting from the top ranked rec (again)\n",
    "    if source in list(undiverse_sources_with_target.keys()):\n",
    "        if source not in list(action_recs_per_former_undiverse_source.keys()):\n",
    "            action_recs_per_former_undiverse_source[source] = []\n",
    "        if top_rec_sub not in action_recs_per_former_undiverse_source[source]:\n",
    "            action_recs_per_former_undiverse_source[source].append(top_rec_sub)\n",
    "\n",
    "more_diverse_action_sources = {a:b for a, b in list(action_recs_per_former_undiverse_source.items()) if len(b) > 1}\n",
    "print(len(more_diverse_action_sources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ground_clove', ['allspice', 'nutmeg']]\n",
      "['cream', ['milk', 'buttermilk']]\n",
      "['olive_oil', ['butter', 'applesauce', 'canola_oil', 'sesame_oil']]\n",
      "['sour_cream', ['yogurt', 'vanilla_yogurt', 'milk']]\n",
      "['salt', ['garlic_salt', 'sugar', 'sea_salt', 'soy_sauce']]\n",
      "['vegetable_broth', ['chicken_broth', 'water']]\n",
      "['stewed_tomato', ['rotel_tomato', 'diced_tomato']]\n",
      "['egg', ['egg_white', 'applesauce']]\n",
      "['cinnamon', ['pumpkin_pie_spice', 'allspice', 'apple_pie_spice', 'ground_ginger', 'chili_powder']]\n",
      "['apple', ['pear', 'applesauce']]\n",
      "['lemon_juice', ['lime_juice', 'orange_juice']]\n",
      "['fish_sauce', ['soy_sauce', 'oyster_sauce']]\n",
      "['green_chilies', ['jalapeno', 'fresh_jalapeno_chilies', 'salsa', 'rotel']]\n",
      "['apple_cider', ['apple_juice', 'chicken_broth']]\n",
      "['chicken_bouillon_cube', ['chicken_broth', 'chicken_stock_cube']]\n",
      "['seasoning_salt', ['garlic_powder', 'garlic_salt', 'onion_salt', 'cajun_seasoning', 'creole_seasoning']]\n",
      "['low_fat_cottage_cheese', ['ricotta_cheese', 'sour_cream']]\n",
      "['low_fat_sour_cream', ['yogurt', 'plain_yogurt']]\n",
      "['beef_broth', ['chicken_broth', 'water']]\n",
      "['ground_cinnamon', ['pumpkin_pie_spice', 'pumpkin_spice']]\n",
      "['thyme', ['oregano', 'rosemary', 'tarragon']]\n",
      "['yellow_cake_mix', ['white_cake_mix', 'cupcake']]\n",
      "['heavy_whipping_cream', ['milk', 'cool_whip']]\n",
      "['half_and_half_cream', ['milk', 'heavy_whipping_cream']]\n",
      "['dry_breadcrumb', ['ritz_cracker', 'pork_rind', 'cracker_crumb']]\n",
      "['snow_pea', ['broccoli', 'sugar_snap_pea']]\n",
      "['pumpkin', ['butternut_squash', 'sweet_potato']]\n",
      "['nutmeg', ['allspice', 'pumpkin_pie_spice']]\n",
      "['kosher_salt', ['sea_salt', 'truffle_salt']]\n",
      "['tomato_juice', ['tomato_sauce', 'diced_tomato']]\n"
     ]
    }
   ],
   "source": [
    "#which ingredients did better?\n",
    "better_sub_pair_names = []\n",
    "for source, distinct_targets in more_diverse_action_sources.items():\n",
    "    target_names = [node_idx_2_name[int(distinct_target)] for distinct_target in distinct_targets if int(distinct_target) in list(node_idx_2_name.keys())]\n",
    "    source_name = node_idx_2_name[int(source)]\n",
    "    better_sub_pair_names.append([source_name, target_names])\n",
    "\n",
    "for names in better_sub_pair_names:\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "olive oil -> sesame oil conventionally might fit better if trying to cook in a east asian style, where the nutty sesame aroma adds a robustness to the taste, especially if butter is not a familiar ingredient\n",
    "however, in an aglio et olio recipe butter might fit better because the less intense taste give the garlic more room to breathe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2853\n"
     ]
    }
   ],
   "source": [
    "print(\"check out those interesting substitutions for olive oil, tho!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foodrecommnederscrapbooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
